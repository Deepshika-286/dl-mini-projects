{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepshika-286/dl-mini-projects/blob/main/DL_all_weeks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Libraries"
      ],
      "metadata": {
        "id": "BHVvpA4IAvMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "X_test= X_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "model.compile(optimizer=SGD(learning_rate=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train,y_train, epochs=10, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngePwhHYkxU1",
        "outputId": "137282fc-4ab5-4017-c58d-c5d5fd13beaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 - 6s - loss: 0.2843 - accuracy: 0.9151 - 6s/epoch - 3ms/step\n",
            "Epoch 2/10\n",
            "1875/1875 - 4s - loss: 0.1189 - accuracy: 0.9642 - 4s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "1875/1875 - 4s - loss: 0.0848 - accuracy: 0.9740 - 4s/epoch - 2ms/step\n",
            "Epoch 4/10\n",
            "1875/1875 - 5s - loss: 0.0656 - accuracy: 0.9794 - 5s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "1875/1875 - 4s - loss: 0.0516 - accuracy: 0.9840 - 4s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "1875/1875 - 5s - loss: 0.0410 - accuracy: 0.9870 - 5s/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "1875/1875 - 4s - loss: 0.0337 - accuracy: 0.9891 - 4s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "1875/1875 - 4s - loss: 0.0268 - accuracy: 0.9917 - 4s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "1875/1875 - 5s - loss: 0.0216 - accuracy: 0.9937 - 5s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "1875/1875 - 4s - loss: 0.0186 - accuracy: 0.9943 - 4s/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79c6d0c25db0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Linear Regression"
      ],
      "metadata": {
        "id": "Z1bH-rXlALK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67LRvMpdAIQE",
        "outputId": "564fda49-1c6a-4eef-de42-9b4a9f83c87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 21.8486\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 17.9763\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 14.8181\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 12.0601\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 10.0532\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 8.2534\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.7796\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.5970\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 4.5385\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.7705\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.0916\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.5821\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.1400\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.7828\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4993\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.2812\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0658\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.9261\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7846\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6552\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5742\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5012\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4466\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3992\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.3600\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.3270\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2971\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2762\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2541\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2361\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2194\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2078\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1948\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1843\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1792\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1739\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1692\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1660\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1622\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1574\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1553\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1521\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1496\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1473\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1455\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1436\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.1422\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1402\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1386\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1370\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1359\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1341\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1329\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1314\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1302\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1288\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1275\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1262\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1251\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1238\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1226\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1214\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1202\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1191\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1180\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1170\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1160\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1147\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1134\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1123\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1109\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1098\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1091\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1079\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1067\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1057\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1047\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1036\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1024\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1015\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1007\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0996\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0989\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0979\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0970\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0961\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0952\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0944\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0936\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0927\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0920\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0912\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0900\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0891\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0882\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0871\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0865\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0856\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0848\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0840\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0568\n",
            "Test loss: 0.05681654065847397\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Predictions: [[4.0251846]\n",
            " [4.3430347]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# Generate random data for training\n",
        "np.random.seed(0)\n",
        "X_train = np.random.rand(100, 1) # Input features\n",
        "y_train = 2 * X_train + 3 + np.random.randn(100, 1) * 0.1 # Target labels\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Dense(units=1, input_shape=(1,))\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
        "# Generate random data for testing\n",
        "X_test = np.random.rand(10, 1) # Input features\n",
        "y_test = 2 * X_test + 3 # Target labels\n",
        "# Evaluate the model on the test data\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', loss)\n",
        "# Predict on new data\n",
        "X_new = np.array([[0.5], [0.8]]) # New input features\n",
        "y_pred = model.predict(X_new)\n",
        "print('Predictions:', y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning Packages Basics: TensorFlow, Keras and PyTorch"
      ],
      "metadata": {
        "id": "u402-LrBAdDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define or load your training and validation data\n",
        "# Replace these with your actual data\n",
        "X_train = np.random.rand(1000, 20)  # Example training data\n",
        "y_train = np.random.randint(0, 2, size=(1000,))  # Example training labels\n",
        "X_val = np.random.rand(200, 20)  # Example validation data\n",
        "y_val = np.random.randint(0, 2, size=(200,))  # Example validation labels\n",
        "\n",
        "# Define your model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=20))  # Modify input_dim accordingly\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with your data\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Make predictions on new data\n",
        "X_test = np.random.rand(50, 20)  # Example new data\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKO9rx0jAhZZ",
        "outputId": "9b5547b3-96b6-4ed6-c911-67e0807ea133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 2s 18ms/step - loss: 0.7044 - accuracy: 0.4770 - val_loss: 0.6970 - val_accuracy: 0.4700\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6980 - accuracy: 0.4990 - val_loss: 0.6961 - val_accuracy: 0.4800\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5110 - val_loss: 0.6974 - val_accuracy: 0.4850\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5190 - val_loss: 0.6977 - val_accuracy: 0.4750\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.5260 - val_loss: 0.6990 - val_accuracy: 0.4700\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5420 - val_loss: 0.6968 - val_accuracy: 0.4850\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5530 - val_loss: 0.6978 - val_accuracy: 0.4750\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5610 - val_loss: 0.6981 - val_accuracy: 0.4750\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5430 - val_loss: 0.7002 - val_accuracy: 0.5050\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6826 - accuracy: 0.5600 - val_loss: 0.6972 - val_accuracy: 0.4950\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4950\n",
            "Validation Loss: 0.6971890926361084\n",
            "Validation Accuracy: 0.4950000047683716\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Predictions: [[0.55428785]\n",
            " [0.5695764 ]\n",
            " [0.4294238 ]\n",
            " [0.46668923]\n",
            " [0.51301426]\n",
            " [0.55321395]\n",
            " [0.4977007 ]\n",
            " [0.5019978 ]\n",
            " [0.45195723]\n",
            " [0.5447153 ]\n",
            " [0.5467251 ]\n",
            " [0.52163374]\n",
            " [0.53262746]\n",
            " [0.50811476]\n",
            " [0.47839573]\n",
            " [0.4651994 ]\n",
            " [0.52225405]\n",
            " [0.40735558]\n",
            " [0.5501486 ]\n",
            " [0.50096446]\n",
            " [0.52435374]\n",
            " [0.39434057]\n",
            " [0.45322406]\n",
            " [0.4924629 ]\n",
            " [0.49151605]\n",
            " [0.44803149]\n",
            " [0.5134479 ]\n",
            " [0.5475467 ]\n",
            " [0.5349795 ]\n",
            " [0.49945652]\n",
            " [0.50343305]\n",
            " [0.5142359 ]\n",
            " [0.556309  ]\n",
            " [0.53346777]\n",
            " [0.48764274]\n",
            " [0.5039393 ]\n",
            " [0.5047909 ]\n",
            " [0.5197544 ]\n",
            " [0.46240157]\n",
            " [0.53144526]\n",
            " [0.515657  ]\n",
            " [0.5284224 ]\n",
            " [0.559363  ]\n",
            " [0.5148291 ]\n",
            " [0.5485938 ]\n",
            " [0.48674324]\n",
            " [0.5143446 ]\n",
            " [0.49608207]\n",
            " [0.5586276 ]\n",
            " [0.49849808]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple computational graph\n",
        "a = tf.constant(2)\n",
        "b = tf.constant(3)\n",
        "c = tf.add(a, b)\n",
        "\n",
        "# Print the result using eager execution\n",
        "print(\"TensorFlow Result:\", c.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUGbYf0eBaQT",
        "outputId": "9192a804-038f-4767-9d9f-1f536ec418a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Result: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using LSTM"
      ],
      "metadata": {
        "id": "Vx3EYg5ZPD1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# Set the maximum number of words to consider in the dataset\n",
        "max_words = 10000\n",
        "# Load the IMDB movie review dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "# Set the maximum sequence length for each review\n",
        "max_sequence_length = 500\n",
        "# Pad sequences to a fixed length\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_sequence_length)\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 32, input_length=max_sequence_length))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMVMJcklPDef",
        "outputId": "afb4116f-34b3-47d5-a614-a986fd32935c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 60s 148ms/step - loss: 0.5124 - accuracy: 0.7430 - val_loss: 0.3566 - val_accuracy: 0.8486\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 31s 79ms/step - loss: 0.2712 - accuracy: 0.8933 - val_loss: 0.2966 - val_accuracy: 0.8791\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.2040 - accuracy: 0.9254 - val_loss: 0.3180 - val_accuracy: 0.8763\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.3180 - accuracy: 0.8763\n",
            "Test Loss: 0.31796687841415405\n",
            "Test Accuracy: 0.8763200044631958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Modeling using RNN"
      ],
      "metadata": {
        "id": "K6pqH-uwOiTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras  # Importing from tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Use pad_sequences from tensorflow.keras\n",
        "\n",
        "# Sample text for training\n",
        "text = \"\"\"This is an example text used for training a word-level RNN language model. You can replace this text with your own dataset.\"\"\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences and target words\n",
        "sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        sequences.append(n_gram_sequence)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Split sequences into input and target\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "y = keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Training the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Generate text using the trained model\n",
        "seed_text = \"This is\"\n",
        "seed_tokens = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "for _ in range(50):\n",
        "    padded_seed = pad_sequences([seed_tokens], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_index = np.argmax(model.predict(padded_seed, verbose=0))\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    seed_tokens.append(predicted_index)\n",
        "    #print(\"************output************\")\n",
        "    print(predicted_word, end=' ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xohsmK9JOf8K",
        "outputId": "daea2970-5038-45e2-ec3f-43f608b6008a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.0912\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0832\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0751\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0665\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.0570\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.0462\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.0333\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0175\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.9974\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9710\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.9365\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8958\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8638\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.8297\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7716\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7150\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6699\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6057\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5257\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4916\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3975\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3820\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2903\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2768\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2057\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1371\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1210\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0198\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0180\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9575\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8756\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9039\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8151\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7653\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7691\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6681\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6759\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6060\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5676\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5628\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4821\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4786\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4633\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3824\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4062\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3873\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3669\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3020\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2511\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2790\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1706\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2077\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1266\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1579\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0914\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0967\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0508\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0442\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0173\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9946\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9820\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9452\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9439\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9051\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9061\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8715\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8612\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8430\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8170\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8111\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7838\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7728\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7595\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7349\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7280\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7130\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6894\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6831\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6729\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6481\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6370\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6332\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6143\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5953\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5843\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5780\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5724\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5574\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5436\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5278\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5156\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5065\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4998\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4992\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5033\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5557\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5635\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6431\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4601\n",
            "an example text for training a word level rnn language language model you can replace this text with your own dataset dataset dataset dataset dataset dataset dataset dataset dataset dataset dataset dataset used for for a level rnn model text with own dataset dataset dataset dataset dataset dataset dataset dataset "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using GRU"
      ],
      "metadata": {
        "id": "0tQji5QeONy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import GRU, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# Set the maximum number of words to consider in the dataset\n",
        "max_words = 10000\n",
        "# Load the IMDB movie review dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "# Set the maximum sequence length for each review\n",
        "max_sequence_length = 500\n",
        "# Pad sequences to a fixed length\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_sequence_length)\n",
        "# Define the GRU model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 32, input_length=max_sequence_length))\n",
        "model.add(GRU(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define a function to perform sentiment analysis on unseen text\n",
        "def predict_sentiment(text):\n",
        "    # Tokenize and preprocess the text\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.fit_on_texts([text])\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "    # Make predictions\n",
        "    sentiment_score = model.predict(sequences)[0]\n",
        "\n",
        "    # Determine sentiment (positive or negative)\n",
        "    sentiment = \"Positive\" if sentiment_score > 0.5 else \"Negative\"\n",
        "\n",
        "    return sentiment, sentiment_score\n",
        "\n",
        "# Example unseen text\n",
        "unseen_text = \"This is a great movie! I loved every moment of it.\"\n",
        "\n",
        "# Perform sentiment analysis on the unseen text\n",
        "sentiment, sentiment_score = predict_sentiment(unseen_text)\n",
        "\n",
        "print(\"input:\",unseen_text)\n",
        "\n",
        "# Print the result\n",
        "print(\"Sentiment:\", sentiment)\n",
        "print(\"Sentiment Score:\", sentiment_score)\n",
        "unseen_text = \"This is a slow movie! I dont like it.\"\n",
        "\n",
        "# Perform sentiment analysis on the unseen text\n",
        "sentiment, sentiment_score = predict_sentiment(unseen_text)\n",
        "\n",
        "print(\"input:\",unseen_text)\n",
        "\n",
        "# Print the result\n",
        "print(\"Sentiment:\", sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2pknp46Mm8L",
        "outputId": "9d75d9ff-f9cb-4f1f-9044-faa8957570ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 57s 140ms/step - loss: 0.4401 - accuracy: 0.7812 - val_loss: 0.3381 - val_accuracy: 0.8569\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 32s 81ms/step - loss: 0.2434 - accuracy: 0.9039 - val_loss: 0.3104 - val_accuracy: 0.8689\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 27s 69ms/step - loss: 0.2052 - accuracy: 0.9231 - val_loss: 0.3637 - val_accuracy: 0.8463\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3637 - accuracy: 0.8463\n",
            "Test Loss: 0.36365848779678345\n",
            "Test Accuracy: 0.8463199734687805\n",
            "1/1 [==============================] - 0s 302ms/step\n",
            "input: This is a great movie! I loved every moment of it.\n",
            "Sentiment: Negative\n",
            "Sentiment Score: [0.36280805]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "input: This is a slow movie! I dont like it.\n",
            "Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with Transfer Learning"
      ],
      "metadata": {
        "id": "1c4g5DsjHj1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Load a pre-trained model (VGG16 in this example)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Freeze the pre-trained model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers on top of the pre-trained model\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz56oo9hHh2W",
        "outputId": "725ddaa2-a135-4b9f-d18f-0323ddac5e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 27s 11ms/step - loss: 1.5121 - accuracy: 0.4600 - val_loss: 1.3408 - val_accuracy: 0.5237\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3188 - accuracy: 0.5356 - val_loss: 1.2831 - val_accuracy: 0.5488\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2611 - accuracy: 0.5552 - val_loss: 1.3023 - val_accuracy: 0.5512\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2182 - accuracy: 0.5679 - val_loss: 1.2326 - val_accuracy: 0.5704\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1860 - accuracy: 0.5819 - val_loss: 1.2572 - val_accuracy: 0.5502\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1610 - accuracy: 0.5893 - val_loss: 1.2016 - val_accuracy: 0.5807\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1414 - accuracy: 0.5974 - val_loss: 1.2344 - val_accuracy: 0.5757\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1266 - accuracy: 0.6012 - val_loss: 1.2009 - val_accuracy: 0.5781\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.1062 - accuracy: 0.6110 - val_loss: 1.1644 - val_accuracy: 0.5875\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 1.0911 - accuracy: 0.6150 - val_loss: 1.1758 - val_accuracy: 0.5909\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1758 - accuracy: 0.5909\n",
            "Test Loss: 1.1757601499557495\n",
            "Test Accuracy: 0.5909000039100647\n"
          ]
        }
      ]
    }
  ]
}